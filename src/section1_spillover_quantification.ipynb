{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ix3/yufeihuang/harsh/miniconda3/envs/ks/lib/python3.11/site-packages/dask/dataframe/__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n",
      "/ix3/yufeihuang/harsh/miniconda3/envs/ks/lib/python3.11/site-packages/anndata/utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_path = '../data/spatial_single_cell_KS_adata.h5ad'\n",
    "adata = sc.read_h5ad(adata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import squidpy as sq\n",
    "\n",
    "\n",
    "def _get_X(adata, layer: Optional[str] = None, use_raw: bool = False):\n",
    "    if use_raw:\n",
    "        if adata.raw is None:\n",
    "            raise ValueError(\"use_raw=True but adata.raw is None.\")\n",
    "        return adata.raw.X\n",
    "    if layer is None:\n",
    "        return adata.X\n",
    "    if layer not in adata.layers:\n",
    "        raise KeyError(f\"layer='{layer}' not in adata.layers\")\n",
    "    return adata.layers[layer]\n",
    "\n",
    "\n",
    "def _dense(X):\n",
    "    return X.toarray() if sp.issparse(X) else np.asarray(X)\n",
    "\n",
    "\n",
    "def compute_cluster_centroids(\n",
    "    adata,\n",
    "    cluster_key: str,\n",
    "    *,\n",
    "    layer: Optional[str] = None,\n",
    "    use_raw: bool = False,\n",
    "    gene_subset: Optional[Iterable[str]] = None,\n",
    "    min_cells_per_cluster: int = 20,\n",
    "    l2_normalize: bool = True,\n",
    ") -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reference-free cluster programs: mean expression per cluster.\n",
    "    Returns (centroids, cluster_names, gene_idx).\n",
    "    \"\"\"\n",
    "    if cluster_key not in adata.obs:\n",
    "        raise KeyError(f\"cluster_key='{cluster_key}' not found in adata.obs\")\n",
    "\n",
    "    X = _get_X(adata, layer=layer, use_raw=use_raw)\n",
    "\n",
    "    # Handle gene subsetting with use_raw\n",
    "    if use_raw:\n",
    "        var_names = adata.raw.var_names\n",
    "    else:\n",
    "        var_names = adata.var_names\n",
    "\n",
    "    if gene_subset is None:\n",
    "        gene_idx = np.arange(X.shape[1], dtype=int)\n",
    "    else:\n",
    "        gene_subset = list(gene_subset)\n",
    "        missing = [g for g in gene_subset if g not in var_names]\n",
    "        if missing:\n",
    "            raise KeyError(f\"{len(missing)} genes missing, e.g. {missing[:5]}\")\n",
    "        gene_idx = var_names.get_indexer(gene_subset).astype(int)\n",
    "\n",
    "    clusters = adata.obs[cluster_key].astype(str).values\n",
    "    names_all = sorted(pd.unique(clusters))\n",
    "\n",
    "    centroids = []\n",
    "    names = []\n",
    "    for c in names_all:\n",
    "        idx = np.where(clusters == c)[0]\n",
    "        if idx.size < min_cells_per_cluster:\n",
    "            continue\n",
    "        # Ensure correct shape: flatten to 1D\n",
    "        mu = _dense(X[idx, :][:, gene_idx]).mean(axis=0).ravel()\n",
    "        centroids.append(mu)\n",
    "        names.append(c)\n",
    "\n",
    "    if len(centroids) == 0:\n",
    "        raise ValueError(\"No clusters passed min_cells_per_cluster.\")\n",
    "\n",
    "    C = np.vstack(centroids)\n",
    "    if l2_normalize:\n",
    "        C = normalize(C, norm=\"l2\", axis=1)\n",
    "    return C, names, gene_idx\n",
    "\n",
    "\n",
    "def compute_secondary_program(\n",
    "    adata,\n",
    "    cluster_key: str,\n",
    "    *,\n",
    "    layer: Optional[str] = None,\n",
    "    use_raw: bool = False,\n",
    "    gene_subset: Optional[Iterable[str]] = None,\n",
    "    min_cells_per_cluster: int = 20,\n",
    "    prefix: str = \"spill\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - {prefix}_secondary_cluster\n",
    "      - {prefix}_secondary_strength (sec_sim - prim_sim)\n",
    "    \"\"\"\n",
    "    C, cluster_names, gene_idx = compute_cluster_centroids(\n",
    "        adata,\n",
    "        cluster_key=cluster_key,\n",
    "        layer=layer,\n",
    "        use_raw=use_raw,\n",
    "        gene_subset=gene_subset,\n",
    "        min_cells_per_cluster=min_cells_per_cluster,\n",
    "        l2_normalize=True,\n",
    "    )\n",
    "\n",
    "    # Get X consistently with how centroids were computed\n",
    "    raw_X = _get_X(adata, layer=layer, use_raw=use_raw)\n",
    "    X = _dense(raw_X[:, gene_idx])\n",
    "    X = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "    sims = X @ C.T  # (n_cells, n_clusters)\n",
    "\n",
    "    col_index = {c: j for j, c in enumerate(cluster_names)}\n",
    "    primary = adata.obs[cluster_key].astype(str).values\n",
    "    prim_idx = np.array([col_index.get(c, -1) for c in primary], dtype=int)\n",
    "\n",
    "    # Initialize with NaN for cells whose primary cluster didn't pass filter\n",
    "    prim_sim = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "    ok = prim_idx >= 0\n",
    "    prim_sim[ok] = sims[np.where(ok)[0], prim_idx[ok]]\n",
    "\n",
    "    # Handle edge case: only one cluster passed filter\n",
    "    if len(cluster_names) < 2:\n",
    "        adata.obs[f\"{prefix}_secondary_cluster\"] = pd.Categorical(\n",
    "            [np.nan] * adata.n_obs\n",
    "        )\n",
    "        adata.obs[f\"{prefix}_secondary_strength\"] = np.full(adata.n_obs, np.nan)\n",
    "        return\n",
    "\n",
    "    sims_masked = sims.copy()\n",
    "    for i in np.where(ok)[0]:\n",
    "        sims_masked[i, prim_idx[i]] = -np.inf\n",
    "\n",
    "    # For cells not in valid clusters, mask all similarities\n",
    "    sims_masked[~ok, :] = -np.inf\n",
    "\n",
    "    sec_idx = np.argmax(sims_masked, axis=1)\n",
    "    sec_sim = sims_masked[np.arange(adata.n_obs), sec_idx]\n",
    "\n",
    "    # Handle cells where all similarities are -inf\n",
    "    valid_sec = np.isfinite(sec_sim)\n",
    "    sec_cluster = np.array([None] * adata.n_obs, dtype=object)\n",
    "    sec_cluster[valid_sec] = np.array(cluster_names, dtype=object)[sec_idx[valid_sec]]\n",
    "\n",
    "    sec_strength = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "    valid_both = valid_sec & ok\n",
    "    sec_strength[valid_both] = sec_sim[valid_both] - prim_sim[valid_both]\n",
    "\n",
    "    adata.obs[f\"{prefix}_primary_cluster\"] = adata.obs[cluster_key].astype(str)\n",
    "    adata.obs[f\"{prefix}_primary_strength\"] = prim_sim\n",
    "    adata.obs[f\"{prefix}_secondary_cluster\"] = pd.Categorical(sec_cluster, categories=cluster_names)\n",
    "    adata.obs[f\"{prefix}_secondary_similarity\"] = sec_sim\n",
    "    adata.obs[f\"{prefix}_secondary_strength\"] = sec_strength\n",
    "\n",
    "\n",
    "def _row_normalize_sparse(W: sp.spmatrix) -> sp.csr_matrix:\n",
    "    W = W.tocsr()\n",
    "    rs = np.asarray(W.sum(axis=1)).ravel()\n",
    "    rs[rs == 0] = 1.0\n",
    "    inv = 1.0 / rs\n",
    "    Dinv = sp.diags(inv)\n",
    "    return (Dinv @ W).tocsr()\n",
    "\n",
    "\n",
    "def neighborhood_consistency_squidpy(\n",
    "    adata,\n",
    "    *,\n",
    "    cluster_key: str,\n",
    "    spatial_key: str = \"spatial\",\n",
    "    n_neighs: int = 30,\n",
    "    coord_type: str = \"generic\",\n",
    "    prefix: str = \"spill\",\n",
    "    layer: Optional[str] = None,\n",
    "    use_raw: bool = False,\n",
    "    gene_subset: Optional[Iterable[str]] = None,\n",
    "    min_cells_per_cluster: int = 20,\n",
    "    n_permutations: int = 0,\n",
    "    random_state: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Builds Squidpy spatial graph and computes neighborhood-consistency spillover tests.\n",
    "\n",
    "    Requires:\n",
    "      - adata.obsm[spatial_key] exists (or use squidpy's standard spatial metadata)\n",
    "\n",
    "    Produces in adata.obs:\n",
    "      - {prefix}_secondary_cluster\n",
    "      - {prefix}_secondary_strength\n",
    "      - {prefix}_neigh_secondary_enrichment\n",
    "      - {prefix}_neigh_match_rate\n",
    "      - optional p-values if n_permutations>0:\n",
    "        {prefix}_p_enrichment, {prefix}_p_match\n",
    "    \"\"\"\n",
    "    # Validate spatial coordinates exist\n",
    "    if spatial_key not in adata.obsm:\n",
    "        raise KeyError(f\"spatial_key='{spatial_key}' not found in adata.obsm\")\n",
    "\n",
    "    # 1) Secondary program direction (reference-free)\n",
    "    compute_secondary_program(\n",
    "        adata,\n",
    "        cluster_key=cluster_key,\n",
    "        layer=layer,\n",
    "        use_raw=use_raw,\n",
    "        gene_subset=gene_subset,\n",
    "        min_cells_per_cluster=min_cells_per_cluster,\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "    # 2) Build spatial graph once (fast) and reuse\n",
    "    sq.gr.spatial_neighbors(\n",
    "        adata,\n",
    "        spatial_key=spatial_key,\n",
    "        n_neighs=n_neighs,\n",
    "        coord_type=coord_type,\n",
    "    )\n",
    "    # Squidpy stores connectivities in adata.obsp['spatial_connectivities']\n",
    "    if \"spatial_connectivities\" not in adata.obsp:\n",
    "        raise KeyError(\"Expected adata.obsp['spatial_connectivities'] after sq.gr.spatial_neighbors.\")\n",
    "    W = adata.obsp[\"spatial_connectivities\"].tocsr()\n",
    "    Wn = _row_normalize_sparse(W)\n",
    "\n",
    "    # 3) Neighbor enrichment: neighbors' PRIMARY labels == cell's secondary cluster\n",
    "    primary = adata.obs[cluster_key].astype(str).values\n",
    "    secondary = adata.obs[f\"{prefix}_secondary_cluster\"].astype(str).values\n",
    "\n",
    "    # Build one-hot for primary clusters\n",
    "    prim_cats = pd.Categorical(primary)\n",
    "    prim_codes = prim_cats.codes\n",
    "    nC = len(prim_cats.categories)\n",
    "    P = sp.csr_matrix(\n",
    "        (np.ones(adata.n_obs), (np.arange(adata.n_obs), prim_codes)),\n",
    "        shape=(adata.n_obs, nC)\n",
    "    )\n",
    "\n",
    "    # Neighbor composition per cell in primary-label space\n",
    "    neigh_comp = Wn @ P  # (n_cells, n_clusters)\n",
    "\n",
    "    # For each cell, pick the column corresponding to its secondary label\n",
    "    cat_to_col = {c: j for j, c in enumerate(prim_cats.categories)}\n",
    "    sec_cols = np.array([cat_to_col.get(s, -1) for s in secondary], dtype=int)\n",
    "\n",
    "    enrich = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "    ok = sec_cols >= 0\n",
    "    if ok.any():\n",
    "        # Convert sparse result to dense for indexing\n",
    "        enrich_vals = np.asarray(neigh_comp[np.where(ok)[0], sec_cols[ok]])\n",
    "        # Handle both 1D and 2D array returns\n",
    "        enrich[ok] = enrich_vals.ravel()\n",
    "\n",
    "    adata.obs[f\"{prefix}_neigh_secondary_enrichment\"] = enrich\n",
    "\n",
    "    # 4) Neighbor match rate: neighbors share the same *secondary direction*\n",
    "    # Filter out invalid secondary assignments (NaN, empty string)\n",
    "    valid_secondary = (secondary != \"nan\") & (secondary != \"\") & pd.notna(secondary)\n",
    "    \n",
    "    if valid_secondary.sum() > 0:\n",
    "        sec_filtered = secondary[valid_secondary]\n",
    "        sec_cats = pd.Categorical(sec_filtered)\n",
    "        sec_codes_full = np.full(adata.n_obs, -1, dtype=int)\n",
    "        \n",
    "        # Map secondary categories to codes\n",
    "        sec_cat_to_code = {c: j for j, c in enumerate(sec_cats.categories)}\n",
    "        for i in np.where(valid_secondary)[0]:\n",
    "            sec_codes_full[i] = sec_cat_to_code.get(secondary[i], -1)\n",
    "        \n",
    "        nS = len(sec_cats.categories)\n",
    "        # Only include valid cells in the one-hot matrix\n",
    "        valid_idx = np.where(sec_codes_full >= 0)[0]\n",
    "        S = sp.csr_matrix(\n",
    "            (np.ones(len(valid_idx)), (valid_idx, sec_codes_full[valid_idx])),\n",
    "            shape=(adata.n_obs, nS)\n",
    "        )\n",
    "        neigh_sec_comp = Wn @ S\n",
    "        \n",
    "        match = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "        for i in valid_idx:\n",
    "            match[i] = neigh_sec_comp[i, sec_codes_full[i]]\n",
    "    else:\n",
    "        match = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "        sec_codes_full = np.full(adata.n_obs, -1, dtype=int)\n",
    "        nS = 0\n",
    "\n",
    "    adata.obs[f\"{prefix}_neigh_match_rate\"] = match\n",
    "\n",
    "    # 5) Optional permutation p-values (vectorized; still costs O(B * nnz(W)))\n",
    "    if n_permutations and n_permutations > 0:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "\n",
    "        # enrichment null: shuffle primary labels\n",
    "        enrich_null = np.zeros((n_permutations, adata.n_obs), dtype=float)\n",
    "        base_codes = prim_codes.copy()\n",
    "\n",
    "        for b in range(n_permutations):\n",
    "            perm = base_codes.copy()\n",
    "            rng.shuffle(perm)\n",
    "            Pp = sp.csr_matrix(\n",
    "                (np.ones(adata.n_obs), (np.arange(adata.n_obs), perm)),\n",
    "                shape=(adata.n_obs, nC)\n",
    "            )\n",
    "            comp_p = Wn @ Pp\n",
    "            tmp = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "            ok_idx = np.where(ok)[0]\n",
    "            if len(ok_idx) > 0:\n",
    "                tmp[ok] = np.asarray(comp_p[ok_idx, sec_cols[ok]]).ravel()\n",
    "            enrich_null[b] = tmp\n",
    "\n",
    "        # Vectorized p-value computation\n",
    "        p_enrich = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "        finite_mask = np.isfinite(enrich)\n",
    "        if finite_mask.any():\n",
    "            # Count how many null values >= observed (vectorized)\n",
    "            counts = (enrich_null[:, finite_mask] >= enrich[finite_mask]).sum(axis=0)\n",
    "            p_enrich[finite_mask] = (counts + 1.0) / (n_permutations + 1.0)\n",
    "        adata.obs[f\"{prefix}_p_enrichment\"] = p_enrich\n",
    "\n",
    "        # match null: shuffle secondary directions (only among valid cells)\n",
    "        if nS > 0 and valid_secondary.sum() > 1:\n",
    "            match_null = np.zeros((n_permutations, adata.n_obs), dtype=float)\n",
    "            valid_codes = sec_codes_full[sec_codes_full >= 0]\n",
    "            valid_idx = np.where(sec_codes_full >= 0)[0]\n",
    "            \n",
    "            for b in range(n_permutations):\n",
    "                perm_codes = valid_codes.copy()\n",
    "                rng.shuffle(perm_codes)\n",
    "                \n",
    "                # Rebuild sparse matrix with permuted codes\n",
    "                Sp = sp.csr_matrix(\n",
    "                    (np.ones(len(valid_idx)), (valid_idx, perm_codes)),\n",
    "                    shape=(adata.n_obs, nS)\n",
    "                )\n",
    "                comp_p = Wn @ Sp\n",
    "                \n",
    "                tmp = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "                for j, i in enumerate(valid_idx):\n",
    "                    tmp[i] = comp_p[i, perm_codes[j]]\n",
    "                match_null[b] = tmp\n",
    "\n",
    "            p_match = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "            finite_match = np.isfinite(match)\n",
    "            if finite_match.any():\n",
    "                counts = (match_null[:, finite_match] >= match[finite_match]).sum(axis=0)\n",
    "                p_match[finite_match] = (counts + 1.0) / (n_permutations + 1.0)\n",
    "        else:\n",
    "            p_match = np.full(adata.n_obs, np.nan, dtype=float)\n",
    "        \n",
    "        adata.obs[f\"{prefix}_p_match\"] = p_match\n",
    "\n",
    "\n",
    "def neighborhood_consistency_by_sample(\n",
    "    adata,\n",
    "    *,\n",
    "    sample_key: str = \"sample_id\",\n",
    "    cluster_key: str,\n",
    "    spatial_key: str = \"spatial\",\n",
    "    n_neighs: int = 30,\n",
    "    coord_type: str = \"generic\",\n",
    "    prefix: str = \"spill\",\n",
    "    layer: Optional[str] = None,\n",
    "    use_raw: bool = False,\n",
    "    gene_subset: Optional[Iterable[str]] = None,\n",
    "    min_cells_per_cluster: int = 20,\n",
    "    n_permutations: int = 0,\n",
    "    random_state: int = 0,\n",
    "    verbose: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run neighborhood_consistency_squidpy separately for each sample.\n",
    "    \n",
    "    This avoids building spatial graphs across sample boundaries,\n",
    "    which would create spurious neighbors between unrelated tissues.\n",
    "    \"\"\"\n",
    "    if sample_key not in adata.obs:\n",
    "        raise KeyError(f\"sample_key='{sample_key}' not found in adata.obs\")\n",
    "    \n",
    "    samples = adata.obs[sample_key].unique()\n",
    "    \n",
    "    # Initialize output columns as object/float (not Categorical to avoid category mismatch)\n",
    "    output_cols = [\n",
    "        f\"{prefix}_primary_cluster\",\n",
    "        f\"{prefix}_primary_strength\", \n",
    "        f\"{prefix}_secondary_cluster\",\n",
    "        f\"{prefix}_secondary_similarity\",\n",
    "        f\"{prefix}_secondary_strength\",\n",
    "        f\"{prefix}_neigh_secondary_enrichment\",\n",
    "        f\"{prefix}_neigh_match_rate\",\n",
    "    ]\n",
    "    if n_permutations > 0:\n",
    "        output_cols.extend([f\"{prefix}_p_enrichment\", f\"{prefix}_p_match\"])\n",
    "    \n",
    "    for col in output_cols:\n",
    "        if col.endswith(\"_cluster\"):\n",
    "            adata.obs[col] = pd.Series([None] * adata.n_obs, index=adata.obs.index, dtype=object)\n",
    "        else:\n",
    "            adata.obs[col] = pd.Series(np.nan, index=adata.obs.index, dtype=float)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if verbose:\n",
    "            print(f\"Processing sample {i+1}/{len(samples)}: {sample}\")\n",
    "        \n",
    "        mask = adata.obs[sample_key] == sample\n",
    "        adata_sub = adata[mask].copy()\n",
    "        \n",
    "        if adata_sub.n_obs < min_cells_per_cluster:\n",
    "            if verbose:\n",
    "                print(f\"  Skipping: only {adata_sub.n_obs} cells\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            neighborhood_consistency_squidpy(\n",
    "                adata_sub,\n",
    "                cluster_key=cluster_key,\n",
    "                spatial_key=spatial_key,\n",
    "                n_neighs=n_neighs,\n",
    "                coord_type=coord_type,\n",
    "                prefix=prefix,\n",
    "                layer=layer,\n",
    "                use_raw=use_raw,\n",
    "                gene_subset=gene_subset,\n",
    "                min_cells_per_cluster=min_cells_per_cluster,\n",
    "                n_permutations=n_permutations,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            \n",
    "            # Copy results back to main adata\n",
    "            idx = adata.obs.index[mask]\n",
    "            for col in output_cols:\n",
    "                if col in adata_sub.obs:\n",
    "                    # Convert to numpy array to avoid Categorical issues\n",
    "                    values = adata_sub.obs[col].values\n",
    "                    if hasattr(values, 'to_numpy'):\n",
    "                        values = values.to_numpy()\n",
    "                    # Use numpy array assignment via .loc\n",
    "                    adata.obs.loc[idx, col] = np.asarray(values)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert cluster columns to Categorical at the end\n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    for col in output_cols:\n",
    "        if col.endswith(\"_cluster\"):\n",
    "            adata.obs[col] = pd.Categorical(adata.obs[col])\n",
    "\n",
    "\n",
    "def classify_spillover_vs_hybrid(\n",
    "    adata,\n",
    "    *,\n",
    "    prefix: str = \"spill\",\n",
    "    strength_q: float = 0.90,\n",
    "    enrich_q: float = 0.90,\n",
    "    p_thresh: float = 0.05,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Labels:\n",
    "      - spillover_likely: strong secondary + neighborhood enrichment (and p<=p_thresh if available)\n",
    "      - hybrid_candidate: strong secondary but not enriched\n",
    "      - clean: else\n",
    "    \"\"\"\n",
    "    strength_col = f\"{prefix}_secondary_strength\"\n",
    "    enrich_col = f\"{prefix}_neigh_secondary_enrichment\"\n",
    "    \n",
    "    if strength_col not in adata.obs:\n",
    "        raise KeyError(f\"Column '{strength_col}' not found. Run neighborhood_consistency_squidpy first.\")\n",
    "    if enrich_col not in adata.obs:\n",
    "        raise KeyError(f\"Column '{enrich_col}' not found. Run neighborhood_consistency_squidpy first.\")\n",
    "    \n",
    "    s = adata.obs[strength_col].astype(float).values\n",
    "    e = adata.obs[enrich_col].astype(float).values\n",
    "\n",
    "    # Handle all-NaN case\n",
    "    if np.all(np.isnan(s)) or np.all(np.isnan(e)):\n",
    "        adata.obs[f\"{prefix}_class\"] = pd.Categorical([\"clean\"] * adata.n_obs)\n",
    "        return adata.obs[f\"{prefix}_class\"]\n",
    "\n",
    "    s_thr = np.nanquantile(s, strength_q)\n",
    "    e_thr = np.nanquantile(e, enrich_q)\n",
    "\n",
    "    # Use NaN-safe comparisons\n",
    "    spill = (s >= s_thr) & (e >= e_thr)\n",
    "    if f\"{prefix}_p_enrichment\" in adata.obs:\n",
    "        p = adata.obs[f\"{prefix}_p_enrichment\"].astype(float).values\n",
    "        spill = spill & (p <= p_thresh)\n",
    "\n",
    "    hybrid = (s >= s_thr) & ~spill & ~np.isnan(s)\n",
    "\n",
    "    out = np.array([\"clean\"] * adata.n_obs, dtype=object)\n",
    "    out[spill] = \"spillover_likely\"\n",
    "    out[hybrid] = \"hybrid_candidate\"\n",
    "    adata.obs[f\"{prefix}_class\"] = pd.Categorical(out)\n",
    "    return adata.obs[f\"{prefix}_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 1/16: KS_TMA_1_0026870\n",
      "Processing sample 2/16: KS_TMA_2_0026882\n",
      "Processing sample 3/16: KS_TMA_3_0027198\n",
      "Processing sample 4/16: KS_TMA_4_0026764\n",
      "Processing sample 5/16: KS_TMA_5_0026776\n",
      "Processing sample 6/16: KS_TMA_6_0027092\n",
      "Processing sample 7/16: KS_TMA_7_0027079\n",
      "Processing sample 8/16: KS_TMA_8_0027273\n",
      "Processing sample 9/16: KS_TMA_9_0026831\n",
      "Processing sample 10/16: KS_TMA_10_0026828\n",
      "Processing sample 11/16: KS_TMA_11_0026930\n",
      "Processing sample 12/16: KS_TMA_12_0026888\n",
      "Processing sample 13/16: KS_TMA_13_0027077\n",
      "Processing sample 14/16: KS_TMA_14_0027019\n",
      "Processing sample 15/16: KS_TMA_15_0033811\n",
      "Processing sample 16/16: KS_TMA_16_0033809\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "neighborhood_consistency_by_sample(\n",
    "    adata,\n",
    "    sample_key=\"sample_id\",        # column with sample IDs\n",
    "    cluster_key=\"broad_cell_types\",       # your annotation column\n",
    "    spatial_key=\"spatial\",\n",
    "    n_neighs=30,\n",
    "    n_permutations=0,            # optional, set to 0 to skip\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad('../data/spatial_single_cell_KS_adata_spillover.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated spillover: 216,529 cells (3.7%)\n"
     ]
    }
   ],
   "source": [
    "expected_by_chance = 1 / 16  # 0.0625\n",
    "\n",
    "spillover_mask = (\n",
    "    (adata.obs[\"spill_secondary_strength\"] > 0) &\n",
    "    (adata.obs[\"spill_neigh_secondary_enrichment\"] > expected_by_chance * 2) &\n",
    "    (adata.obs[\"spill_neigh_match_rate\"] > expected_by_chance * 2)\n",
    ")\n",
    "\n",
    "n_spillover = spillover_mask.sum()\n",
    "pct_spillover = 100 * n_spillover / adata.n_obs\n",
    "\n",
    "print(f\"Estimated spillover: {n_spillover:,} cells ({pct_spillover:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
